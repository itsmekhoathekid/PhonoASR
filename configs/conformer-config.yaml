model:
  enc: 
    input_dim: 80
    encoder_dim: 256
    num_attention_heads: 8
    feed_forward_expansion_factor : 4
    dropout_rate: 0.1
    conv_kernel_size: 31
    num_encoder_layers: 1
    output_dim: 256
    in_channels: 1
    out_channels: 32
    name : 'Conformer'
    d_model: 256
  dec: 
    n_layer: 1
    d_model: 256
    ff_size: 1024
    n_head: 4
    dropout: 0.1
    k: 1
  model_name : 'Conformer'
  k: 1

training:
  epochs: 100
  batch_size: 8
  accumulation_steps: 1
  save_path: "/home/anhkhoa/PhonoASR/saves"
  train_path : "/home/anhkhoa/transformer_transducer_speeQ/data/train_w2i.json"
  dev_path : "/home/anhkhoa/transformer_transducer_speeQ/data/test_w2i.json"
  test_path : "/home/anhkhoa/transformer_transducer_speeQ/data/test_w2i.json"
  vocab_path : "/home/anhkhoa/transformer_transducer_speeQ/data/vocab_w2i.json"
  logg : "/home/anhkhoa/TASA/logs/Conformer.log"
  logg : "/home/anhkhoa/TASA/logs/Conformer_infer.log"
  reload: False
  ctc_weight: 0.4
  type_training: 'ctc-kldiv'

optim:
  # type: sgd
  # lr: 0.0005
  # momentum: 0.9
  # weight_decay: 0
  # begin_to_adjust_lr: 60
  # nesterov: None
  # decay_rate: 0.5

  type: adam
  lr: 0.001
  weight_decay: 0.0
  decay_rate: 0.5

scheduler:
  lr_initial: 0.001
  n_warmup_steps: 15000

loss:


  