{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f25b006",
   "metadata": {},
   "source": [
    "## Phoneme Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f497fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words: 116, Percentage: 3.78%\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "def load_json(path):\n",
    "    \"\"\"\n",
    "    Load a json file and return the content as a dictionary.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"r\", encoding= 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_unique_tokens(data):\n",
    "    \"\"\"\n",
    "    Get unique tokens from the dataset.\n",
    "    \"\"\"\n",
    "    unique_tokens = set()\n",
    "    for item in data:\n",
    "        transcription = item[\"text\"]\n",
    "        for token in transcription.split():\n",
    "            unique_tokens.add(token)\n",
    "    return list(unique_tokens)\n",
    "\n",
    "testset_path= \"../dataset/LSVSC_test_word.json\"\n",
    "vocab_train_path = \"../dataset/word_vocab_lsvsc.json\"\n",
    "\n",
    "vocab_train = load_json(vocab_train_path)\n",
    "unique_token_test = get_unique_tokens(load_json(testset_path))\n",
    "oov_word = []\n",
    "for token in unique_token_test:\n",
    "    if token not in vocab_train:\n",
    "        oov_word.append(token)\n",
    "\n",
    "print(f\"Number of OOV words: {len(oov_word)}, Percentage: {len(oov_word)/len(unique_token_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9393d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold phoneme: tu˧˥ kwiən- tʰəː˧˩ ʝaːi˨˩ tʰɯən˨˩ tʰɯən~˧ˀ˩ | pred phonem: tu˧˥ kwiən- tʰəː˧˩ ʝaːi˨˩ tʰɯən˨˩ tʰɯən~˧ˀ˩\n",
      "Gold phoneme: haːi- taːi- t͡ɕoŋ˧˥ naːŋ̟˧ˀ˩ han˧˥ dɯŋ˧˥ ʝaːŋ˧ˀ˩ haːŋ˧˥ ʈ͡ʂɯəŋ~˧˥ fɔŋ˨˩ laːm˨˩ viəŋ~˧ˀ˩ | pred phonem: haːi- taːi- t͡ɕoŋ˧˥ naːŋ̟˧ˀ˩ han˧˥ dɯŋ˧˥ ʝaːŋ˧ˀ˩ haːŋ˧˥ ʈ͡ʂɯəŋ~˧˥ fɔŋ˨˩ laːm˨˩ viəŋ~˧ˀ˩\n",
      "Gold phoneme: dem- vɯə˨˩ roi˨˩ mon~˧ˀ˩ ten- ʈ͡ʂɔŋ- daːm˧˥ zɔi˨˩ bɔ˧ˀ˩ daː˧ˀ˥ t͡ɕen~˧˥ ve˨˩ taːi- han˧˥ | pred phonem: dem- vɯə˨˩ roi˨˩ mon~˧ˀ˩ ten- ʈ͡ʂɔŋ- daːm˧˥ zɔi˨˩ bɔ˧ˀ˩ daː˧ˀ˥ t͡ɕen~˧˥ ve˨˩ taːi- han˧˥\n",
      "Gold phoneme: dem- vɯə˨˩ roi˨˩ mon~˧ˀ˩ ten- ʈ͡ʂɔŋ- daːm˧˥ zɔi˨˩ bɔ˧ˀ˩ daː˧ˀ˥ t͡ɕen~˧˥ ve˨˩ taːi- han˧˥ | pred phonem: dem- vɯə˨˩ roi˨˩ mon~˧ˀ˩ ten- ʈ͡ʂɔŋ- daːm˧˥ zɔi˨˩ bɔ˧ˀ˩ daː˧ˀ˥ t͡ɕen~˧˥ ve˨˩ taːi- han˧˥\n",
      "Gold phoneme: mɛ˧ˀ˩ vaː˨˩ t͡ɕi˧ˀ˩ nɔ˧˥ daːŋ- ŋoi˨˩ vaː˧˥ lɯəi˧˥ ben- məi˧˥ t͡ɕiəŋ~˧˥ tʰwiən˨˩ tʰuŋ˧˥ voi˧ˀ˩ vaːŋ˨˩ t͡ɕaːi˧ˀ˩ len- t͡ɕo˧ˀ˥ hɯŋ- nam˨˩ | pred phonem: mɛ˧ˀ˩ vaː˨˩ t͡ɕi˧ˀ˩ nɔ˧˥ daːŋ- ŋoi˨˩ vaː˧˥ lɯəi˧˥ ben- məi˧˥ t͡ɕiəŋ~˧˥ tʰwiən˨˩ tʰuŋ˧˥ voi˧ˀ˩ vaːŋ˨˩ t͡ɕaːi˧ˀ˩ len- t͡ɕo˧ˀ˥ hɯŋ- nam˨˩\n",
      "Gold phoneme: kuŋ˧ˀ˥ ɲɯ- ɲɯŋ˧ˀ˥ ŋɯəi˨˩ kɔn˨˩ ŋəː˨˩ vɯŋ~˧ˀ˩ χaːŋ~˧˥ ʈ͡ʂɔŋ- daːm˧˥ doŋ- nullaːŋ̟- daː˧ˀ˥ ɲin˨˩ tʰəi˧˥ rɔ˧ˀ˥ kaːi˧˥ ʈ͡ʂɔ˨˩ mi˧ˀ˩ ʝən- kuə˧˩ ten- t͡ɕi˧˩ hwi- ʈ͡ʂɯəŋ˧˩ | pred phonem: kuŋ˧ˀ˥ ɲɯ- ɲɯŋ˧ˀ˥ ŋɯəi˨˩ kɔn˨˩ ŋəː˨˩ vɯŋ~˧ˀ˩ χaːŋ~˧˥ ʈ͡ʂɔŋ- daːm˧˥ doŋ- nullaːŋ̟- daː˧ˀ˥ ɲin˨˩ tʰəi˧˥ rɔ˧ˀ˥ kaːi˧˥ ʈ͡ʂɔ˨˩ mi˧ˀ˩ ʝən- kuə˧˩ ten- t͡ɕi˧˩ hwi- ʈ͡ʂɯəŋ˧˩\n",
      "Number of OOV words: 116, Percentage: 3.78%\n",
      "Number of <unk> in transcript of test dataset: 131\n",
      "Number of correct fill by phoneme based model: 34\n",
      "Percentage of correct fill by phoneme based model: 25.95%\n",
      "Correctly filled words: {'ŋun~˧˥', 'ʈ͡ʂɛu˧˩', 'ɲaːm~˧˥', 'bɔ˧ˀ˩', 'tʰɔm~˧˥', 'ŋɔn˨˩', 'nəŋ˧˥', 'lan~˧ˀ˩', 'ɣen~˧˥', 'maːn~˧ˀ˩', 'nəːm-', 'vaːŋ̟˧ˀ˩', 'ʂɔn˨˩', 'vɔi-', 't͡ɕwaːŋ˧˩', 'tʰwi˧˩', 'tʰɯən˨˩', 'ɲɯ˧˩', 'vaːi˧˥', 'biu˧˥', 'zɔi˨˩', 'muəŋ˧˥', 'ki˧ˀ˩', 't͡ɕwɛ˧˥', 'χɯən~˧˥', 'haːŋ˧˥', 'tʰuŋ˧˥', 'həːn˧˥', 'hwɛ˨˩', 'kwiən~˧˥', 'mi˧ˀ˩', 'ŋaːi˧˥', 'son˧˥'}\n",
      "Cannot be filled words: {'ʈ͡ʂɯəŋ˧ˀ˩', 'ʂiə˧ˀ˩', 'nɯu-', 'faːm˧˩', 't͡ɕɔŋ~˧˥', 'nullɯŋ˧ˀ˥', 'haːŋ~˧ˀ˩', 'nullui-', 'lɯə-', 'maːn~˧ˀ˩', 'ɣwəi-', 'vɔi-', 'san˧˥', 't͡ɕaːn~˧ˀ˩', 'biu˧˥', 'ʂɔːŋ~˧˥', 'tʰɯk-', 'kəŋ~˧˥', 'ʂəi˧˩', 't͡ɕe˧ˀ˥', 'χaŋ˧˥', 'lu˨˩', 'lwaːn~˧˥', 'he˧ˀ˥', 'vɯə-', 'tʰiə-', 'ren˨˩', 'ɣaːp-', 'kiəŋ˨˩', 'χəːi˧˥', 'raːn˧˥', 'ʂiu˧˩', 'vaːm˧ˀ˩', 'ɲan˧ˀ˩', 'ko˨˩', 'tʰum-', 'kɔːŋ-', 'raŋ˧˥', 'ʈ͡ʂɔn~˧˥', 'fi˧ˀ˩', 't͡ɕum-', 'səu-', 'non˧ˀ˩', 'ruŋ˧˥', 'ɲəm˧˩', 'ɣi˨˩', 'ʂaːt-', 'ɣəː-', 'tɯn~˧˥', 'sun-', 't͡ɕɯəŋ˨˩', 'ŋen˧˩', 'ŋin-', 'sum-', 'huə˧˩', 'tʰwi˧˩', 'tʰaːi˧˥', 'saːi-', 'χɯən~˧˥', 'veu-', 'hut-', 'mɯə˧˥', 'fɛŋ-', 'vaːn~˧˥', 'kɯəŋ~˧ˀ˩', 'ɣəːm˨˩', 'ruəŋ˨˩', 'rəːm˨˩', 'kiəŋ˧ˀ˥', 'rɯəi˧˥', 'tʰwəː˧˩', 'bɯəŋ-', 'kiəu˧ˀ˩', 'ʝiu˨˩', 'tʰum˨˩', 'muəi˨˩', 'mɛn~˧ˀ˩', 'nən˧˥', 'ʝik̟-', 'hon˧˩', 'laːn˧ˀ˩', 'nullwan˨˩', 'bɯn-', 'ŋwɛu˧ˀ˩', 'vɔi˧˥', 'nəːm˧˥', 't͡ɕaːŋ-'}\n",
      "Tried to fill but wrong predictions: {'laːŋ˧ˀ˩', 't͡ɕaːn~˧˥', 'tʰaːn~˧ˀ˩', 'ki˨˩', 'mon~˧ˀ˩', 'ŋɛu˧ˀ˩', 'lɯəi˧˥', 'dəːn˨˩', 'ɲəm˨˩', 'riŋ̟˨˩', 't͡ɕi˧˥', 'daːŋ̟˧˥', 'χaŋ˧˩', 'kiəu˨˩', 'siu˧˩', 'kwaːn˧˩', 'ɲən˧ˀ˩', 'ʝaːŋ˧˥', 'su-', 'ho˨˩', 'vəːi˧˥', 'vaːi-', 'kɯə~˧ˀ˩', 'tiəm-', 'vɯə˨˩', 'lo-', 'nulləː-', 'ʈ͡ʂun-', 'tʰi-', 't͡ɕan~˧ˀ˩', 'nullu-', 'faŋ-', 'mɯəŋ-', 'tiən~˧˥', 'maːŋ~˧ˀ˩', 'təm~˧ˀ˩', 'baːn~˧ˀ˩', 'səː˨˩', 'ɣaːm~˧˥', 're~˧˥', 'ʂəu-', 'tʰuŋ-', 'nulloi-', 'hon˨˩', 'he˧ˀ˩', 't͡ɕan˧ˀ˥', 'kaːŋ~˧˥', 'don˧ˀ˩', 'raŋ˨˩', 'biəu˧˥', 'ʂaːn~˧˥', 'ɣaːm-', 'mon˧˩', 'kwəi-', 't͡ɕaŋ~˧˥', 'baː-', 'noŋ-', 'vaː˨˩', 'vi˧ˀ˩', 'tuŋ~˧ˀ˩', 'kɔn-', 'tʰɯŋ~˧ˀ˩', 'tʰɔ˧˩', 'ŋan˧˥', 'ʂɔn~˧˥', 'laː˨˩', 'kuə˧˩', 'ʂan˧˥', 'vaːm˨˩', 'ʂiu˨˩', 'kɯu˧ˀ˥', 'tʰɔ˧˥', 'ʝiŋ̟~˧ˀ˩', 'ʈ͡ʂe˧ˀ˥', 'kəː˨˩', 'hun~˧˥', 'nullwaːi-', 'zuəŋ˨˩', 'nullaːŋ̟-', 'ŋəːi˨˩', 'ʈ͡ʂɔŋ-', 'ʈ͡ʂaːŋ-', 'ʈ͡ʂɯəŋ˨˩', 'de˧ˀ˩', 'siəu-', 'ʂɯəŋ˧ˀ˩', 'raː-', 'vɛu-', 'ŋin˨˩', 'kɔn˨˩', 'lɯu-', 'faːi˧˩', 'sɯ˧˩', 'χuən~˧˥', 'ʝɯəi˧˥', 'nɯəŋ~˧˥'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# num <unk> in test result\n",
    "test_path_result = \"./result/result-tasa-w2i-lsvsc.json\"\n",
    "test_result = load_json(test_path_result)\n",
    "num_unk = 0\n",
    "idx_list_data = []\n",
    "\n",
    "for idx, item in enumerate(test_result):\n",
    "    try:\n",
    "        transcription = item[\"gold\"]\n",
    "        for idx_word, token in enumerate(transcription.split()):\n",
    "            if token == \"<unk>\":\n",
    "                num_unk += 1\n",
    "                idx_list_data.append((idx, idx_word))\n",
    "    except: \n",
    "        continue\n",
    "\n",
    "\n",
    "# num correct fill by phoneme based model \n",
    "\n",
    "test_result_phoneme = load_json(\"./result/result-tasa-p2i_lsvsc.json\")\n",
    "phoneme_test_path = \"../dataset/LSVSC_test_phoneme.json\"\n",
    "phoneme_test_data = load_json(phoneme_test_path)\n",
    "num_correct_fill = 0\n",
    "\n",
    "correct_filled = []\n",
    "cannot_be_filled = []\n",
    "tried_to_filled_but_wrong = []\n",
    "test_data = load_json(testset_path)\n",
    "sample_correct = []\n",
    "total_num = 0\n",
    "for idx, idx_word in idx_list_data:\n",
    "    if idx_word < len(test_result_phoneme[idx][\"predicted\"].split(' ')):\n",
    "        gold = test_result_phoneme[idx][\"gold\"].split(' ')[idx_word]\n",
    "        pred = test_result_phoneme[idx][\"predicted\"].split(' ')[idx_word]\n",
    "        # print(f\"Gold: {gold}, Pred: {pred}\")\n",
    "        if gold == pred:\n",
    "            \n",
    "            correct_filled.append(gold)\n",
    "            num_correct_fill += 1\n",
    "            if test_result_phoneme[idx]['gold'] == test_result_phoneme[idx][\"predicted\"]:\n",
    "                print(f\"Gold phoneme: {test_result_phoneme[idx]['gold']} | pred phonem: {test_result_phoneme[idx][\"predicted\"]}\")\n",
    "\n",
    "                sample_correct.append([\n",
    "                    test_result[idx]['gold'],\n",
    "                    test_data[idx]['text']\n",
    "                ])\n",
    "        else:\n",
    "            cannot_be_filled.append(gold)\n",
    "            tried_to_filled_but_wrong.append(pred)\n",
    "        total_num += 1\n",
    "\n",
    "print(f\"Number of OOV words: {len(oov_word)}, Percentage: {len(oov_word)/len(unique_token_test)*100:.2f}%\")\n",
    "print(f\"Number of <unk> in transcript of test dataset: {total_num}\")\n",
    "print(f\"Number of correct fill by phoneme based model: {num_correct_fill}\")\n",
    "print(f\"Percentage of correct fill by phoneme based model: {num_correct_fill/total_num*100:.2f}%\")\n",
    "print(f\"Correctly filled words: {set(correct_filled)}\")\n",
    "print(f\"Cannot be filled words: {set(cannot_be_filled)}\")\n",
    "print(f\"Tried to fill but wrong predictions: {set(tried_to_filled_but_wrong)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c3613d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold unknown word: tú quyên thở dài <unk> thượt \n",
      "Predicted Phoneme Converted: tú quyên thở dài thườn thượt \n",
      "--------------------\n",
      "Gold unknown word: hai tay chống nạnh hắn đứng dạng <unk> trước phòng làm việc \n",
      "Predicted Phoneme Converted: hai tay chống nạnh hắn đứng dạng háng trước phòng làm việc \n",
      "--------------------\n",
      "Gold unknown word: đêm vừa rồi một tên trong đám <unk> <unk> đã chết về tay hắn \n",
      "Predicted Phoneme Converted: đêm vừa rồi một tên trong đám giòi bọ đã chết về tay hắn \n",
      "--------------------\n",
      "Gold unknown word: mẹ và chị nó đang ngồi vá lưới bên mấy chiếc thuyền <unk> vội vàng chạy lên chỗ hưng nằm \n",
      "Predicted Phoneme Converted: mẹ và chị nó đang ngồi vá lưới bên mấy chiếc thuyền thúng vội vàng chạy lên chỗ hưng nằm \n",
      "--------------------\n",
      "Gold unknown word: cũng như những người còn ngờ vực khác trong đám đông anh đã nhìn thấy rõ cái trò <unk> dân của tên chỉ huy trưởng \n",
      "Predicted Phoneme Converted: cũng như những người còn ngờ vực khác trong đám đông anh đã nhìn thấy rõ cái trò mị dân của tên chỉ huy trưởng \n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "duplicate_check = set()\n",
    "for gold, text in sample_correct:\n",
    "    if (gold, text) not in duplicate_check:\n",
    "        print(f\"Gold unknown word: {gold} \\nPredicted Phoneme Converted: {text} \\n{'-'*20}\")\n",
    "        duplicate_check.add((gold, text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "512e6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(path, data):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a json file.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"w\", encoding= 'utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "save_json(\"./result/correctly_filled_words.json\", list(set(correct_filled)))\n",
    "save_json(\"./result/cannot_be_filled_words.json\", list(set(cannot_be_filled)))\n",
    "save_json(\"./result/tried_to_filled_but_wrong_predictions.json\", list(set(tried_to_filled_but_wrong)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffb4d3",
   "metadata": {},
   "source": [
    "## Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f90ec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens correctly predicted at least 1 times: 2440\n",
      "Total unique tokens in GT: 2957\n",
      "Words used: 2957\n",
      "Pearson r: 0.23126171412486463\n",
      "Spearman rho: 0.3899768691222494\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load unique word vocab \n",
    "# load unique word phoneme \n",
    "\n",
    "def load_json(path):\n",
    "    \"\"\"\n",
    "    Load a json file and return the content as a dictionary.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"r\", encoding= 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def save_json(path, data):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a json file.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"w\", encoding= 'utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "from jiwer import process_words\n",
    "\n",
    "def compute_token_stats_with_jiwer(path, correct_at_least=1, save_path=None):\n",
    "    data = load_json(path)\n",
    "    data = [x for x in data if isinstance(x, dict) and \"gold\" in x and \"predicted\" in x]\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    for item in data:\n",
    "        gold = item[\"gold\"].strip()\n",
    "        pred = item[\"predicted\"].strip()\n",
    "\n",
    "        out = process_words(gold, pred)\n",
    "\n",
    "        aligns = out.alignments[0]\n",
    "\n",
    "        # unwrap references/hypotheses if nested\n",
    "        ref = out.references[0]     # list[str]\n",
    "\n",
    "        for ch in aligns:\n",
    "            op = ch.type\n",
    "            rs, re = ch.ref_start_idx, ch.ref_end_idx\n",
    "\n",
    "            if op == \"insert\":\n",
    "                continue\n",
    "\n",
    "            # for equal/delete/substitute: total increases for ref tokens\n",
    "            for tok in ref[rs:re]:\n",
    "                # tok must be a string token\n",
    "                if isinstance(tok, list):\n",
    "                    # unexpected nesting; flatten\n",
    "                    for t in tok:\n",
    "                        stats.setdefault(t, {\"total\": 0, \"correct\": 0})\n",
    "                        stats[t][\"total\"] += 1\n",
    "                        if op == \"equal\":\n",
    "                            stats[t][\"correct\"] += 1\n",
    "                else:\n",
    "                    stats.setdefault(tok, {\"total\": 0, \"correct\": 0})\n",
    "                    stats[tok][\"total\"] += 1\n",
    "                    if op == \"equal\":\n",
    "                        stats[tok][\"correct\"] += 1\n",
    "\n",
    "    unique_correct = sum(1 for tok, s in stats.items() if s[\"correct\"] >= correct_at_least)\n",
    "\n",
    "    print(f\"Number of unique tokens correctly predicted at least {correct_at_least} times: {unique_correct}\")\n",
    "    print(f\"Total unique tokens in GT: {len(stats)}\")\n",
    "\n",
    "    # stats = {tok: s for tok, s in stats.items() if s[\"correct\"] >= correct_at_least}\n",
    "    if save_path:\n",
    "        save_json(save_path, stats)\n",
    "\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_frequency_training_set(path, save_path = None, type = 'phoneme'):\n",
    "    data = load_json(path)\n",
    "    word_count = {}\n",
    "    for item in data:\n",
    "        \n",
    "        transcription = item[\"phoneme_presentation\"] if type == 'phoneme' else item[\"text\"]\n",
    "        for token in transcription.split(' '):\n",
    "            if token not in word_count:\n",
    "                word_count[token] = 0\n",
    "            word_count[token] += 1\n",
    "    if save_path:\n",
    "        save_json(save_path, word_count)\n",
    "    return word_count\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def build_vectors(test_stats, train_freq, include_zero=False):\n",
    "    freqs, recalls = [], []\n",
    "\n",
    "    for w, s in test_stats.items():\n",
    "        if s[\"total\"] == 0:\n",
    "            continue\n",
    "\n",
    "        recall = s[\"correct\"] / s['total']\n",
    "        freq = train_freq.get(w, 0)\n",
    "        \n",
    "\n",
    "        if not include_zero and freq == 0:\n",
    "            continue\n",
    "\n",
    "        freqs.append(freq)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return np.array(freqs), np.array(recalls)\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "unique_word_count_test_set = compute_token_stats_with_jiwer(\"./result/result-tasa-w2i-lsvsc.json\", correct_at_least=1, save_path=\"./result/unique_word_stats_test_set.json\")\n",
    "unique_word_count_training_set = word_frequency_training_set(\"../dataset/LSVSC_train_word.json\", save_path=\"./result/unique_word_stats_training_set.json\", type = 'word')\n",
    "\n",
    "freqs, recalls = build_vectors(\n",
    "    unique_word_count_test_set,\n",
    "    unique_word_count_training_set,\n",
    "    include_zero=True   # giống paper\n",
    ")\n",
    "\n",
    "pearson_r, _ = pearsonr(freqs, recalls)\n",
    "spearman_rho, _ = spearmanr(freqs, recalls)\n",
    "\n",
    "print(\"Words used:\", len(freqs))\n",
    "print(\"Pearson r:\", pearson_r)\n",
    "print(\"Spearman rho:\", spearman_rho)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d99e654",
   "metadata": {},
   "source": [
    "Result based on TASA p, c, word result\n",
    "\n",
    "## Phoneme\n",
    "Number of unique tokens correctly predicted at least 1 times: 2474\n",
    "Total unique tokens in GT: 3001\n",
    "Words used: 3001\n",
    "Pearson r: 0.22260085725351292\n",
    "Spearman rho: 0.32388628887549537\n",
    "\n",
    "## Character\n",
    "\n",
    "Number of unique tokens correctly predicted at least 1 times: 2700\n",
    "Total unique tokens in GT: 3586\n",
    "Words used: 3586\n",
    "Pearson r: 0.24836971446922626\n",
    "Spearman rho: 0.4811725802044207\n",
    "\n",
    "## Word\n",
    "\n",
    "Number of unique tokens correctly predicted at least 1 times: 2440\n",
    "Total unique tokens in GT: 2957\n",
    "Words used: 2957\n",
    "Pearson r: 0.23126171412486463\n",
    "Spearman rho: 0.3899768691222494"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
