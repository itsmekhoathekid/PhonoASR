{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f25b006",
   "metadata": {},
   "source": [
    "## Phoneme Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59f497fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words: 116, Percentage: 3.78%\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "def load_json(path):\n",
    "    \"\"\"\n",
    "    Load a json file and return the content as a dictionary.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"r\", encoding= 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_unique_tokens(data):\n",
    "    \"\"\"\n",
    "    Get unique tokens from the dataset.\n",
    "    \"\"\"\n",
    "    unique_tokens = set()\n",
    "    for item in data:\n",
    "        transcription = item[\"text\"]\n",
    "        for token in transcription.split():\n",
    "            unique_tokens.add(token)\n",
    "    return list(unique_tokens)\n",
    "\n",
    "testset_path= \"../dataset/LSVSC_test_word.json\"\n",
    "vocab_train_path = \"../dataset/word_vocab_lsvsc.json\"\n",
    "\n",
    "vocab_train = load_json(vocab_train_path)\n",
    "unique_token_test = get_unique_tokens(load_json(testset_path))\n",
    "oov_word = []\n",
    "for token in unique_token_test:\n",
    "    if token not in vocab_train:\n",
    "        oov_word.append(token)\n",
    "\n",
    "print(f\"Number of OOV words: {len(oov_word)}, Percentage: {len(oov_word)/len(unique_token_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9393d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words: 116, Percentage: 3.78%\n",
      "Number of <unk> in transcript of test dataset: 131\n",
      "Number of correct fill by phoneme based model: 34\n",
      "Percentage of correct fill by phoneme based model: 25.95%\n",
      "Correctly filled words: {'hwɛ˨˩', 'tʰuŋ˧˥', 'ɲɯ˧˩', 'son˧˥', 'ki˧ˀ˩', 'χɯən~˧˥', 'lan~˧ˀ˩', 'kwiən~˧˥', 'bɔ˧ˀ˩', 'ŋun~˧˥', 'muəŋ˧˥', 'tʰwi˧˩', 'mi˧ˀ˩', 't͡ɕwɛ˧˥', 'ɲaːm~˧˥', 'ʂɔn˨˩', 'maːn~˧ˀ˩', 'vaːi˧˥', 'tʰɯən˨˩', 'vaːŋ̟˧ˀ˩', 'vɔi-', 'zɔi˨˩', 'nəːm-', 'nəŋ˧˥', 'haːŋ˧˥', 'biu˧˥', 'ɣen~˧˥', 'ŋɔn˨˩', 'ʈ͡ʂɛu˧˩', 'tʰɔm~˧˥', 'həːn˧˥', 't͡ɕwaːŋ˧˩', 'ŋaːi˧˥'}\n",
      "Cannot be filled words: {'ʈ͡ʂɯəŋ˧ˀ˩', 'sum-', 'ɣi˨˩', 'hut-', 'tʰiə-', 'kəŋ~˧˥', 'ŋwɛu˧ˀ˩', 'ʝiu˨˩', 'veu-', 'tʰwəː˧˩', 'mɛn~˧ˀ˩', 'ɣəːm˨˩', 'kɯəŋ~˧ˀ˩', 'rəːm˨˩', 'nullɯŋ˧ˀ˥', 'ʂɔːŋ~˧˥', 'kiəŋ˨˩', 'raːn˧˥', 'ʝik̟-', 'kiəŋ˧ˀ˥', 'maːn~˧ˀ˩', 'laːn˧ˀ˩', 'sun-', 'vɔi˧˥', 'vɔi-', 'hon˧˩', 'ʂiu˧˩', 't͡ɕaːn~˧ˀ˩', 'muəi˨˩', 'ruəŋ˨˩', 'ɣaːp-', 'faːm˧˩', 'tɯn~˧˥', 'bɯəŋ-', 'huə˧˩', 'ʂaːt-', 'kɔːŋ-', 'tʰum-', 'fi˧ˀ˩', 'ʂəi˧˩', 'χəːi˧˥', 'ʈ͡ʂɔn~˧˥', 'nəːm˧˥', 't͡ɕe˧ˀ˥', 'fɛŋ-', 'ren˨˩', 'biu˧˥', 'ɲan˧ˀ˩', 'non˧ˀ˩', 'lu˨˩', 'ɲəm˧˩', 'he˧ˀ˥', 'nullui-', 'χɯən~˧˥', 'nɯu-', 'saːi-', 'ko˨˩', 'tʰwi˧˩', 'san˧˥', 'səu-', 'ɣəː-', 'mɯə˧˥', 'lwaːn~˧˥', 'ʂiə˧ˀ˩', 'vɯə-', 'ŋen˧˩', 'ruŋ˧˥', 'raŋ˧˥', 'vaːn~˧˥', 'tʰɯk-', 'tʰaːi˧˥', 't͡ɕum-', 'χaŋ˧˥', 'haːŋ~˧ˀ˩', 'tʰum˨˩', 'ŋin-', 't͡ɕaːŋ-', 'nən˧˥', 'kiəu˧ˀ˩', 'lɯə-', 'bɯn-', 'ɣwəi-', 'vaːm˧ˀ˩', 'nullwan˨˩', 'rɯəi˧˥', 't͡ɕɔŋ~˧˥', 't͡ɕɯəŋ˨˩'}\n",
      "Tried to fill but wrong predictions: {'ʈ͡ʂɯəŋ˨˩', 'tʰaːn~˧ˀ˩', 'kəː˨˩', 'ʈ͡ʂɔŋ-', 'baːn~˧ˀ˩', 'tuŋ~˧ˀ˩', 'mɯəŋ-', 'ʂəu-', 'nulloi-', 'vɯə˨˩', 'laː˨˩', 'nullwaːi-', 'noŋ-', 'kaːŋ~˧˥', 'χaŋ˧˩', 'de˧ˀ˩', 'ɣaːm-', 'tʰɯŋ~˧ˀ˩', 'vaːm˨˩', 'daːŋ̟˧˥', 'səː˨˩', 'vɛu-', 'siu˧˩', 'hon˨˩', 'vi˧ˀ˩', 'ʝiŋ̟~˧ˀ˩', 'raː-', 't͡ɕaŋ~˧˥', 'laːŋ˧ˀ˩', 'ʈ͡ʂaːŋ-', 'ɲəm˨˩', 'maːŋ~˧ˀ˩', 'ʂɔn~˧˥', 'vaː˨˩', 'faŋ-', 'tʰɔ˧˥', 'mon~˧ˀ˩', 'ʝaːŋ˧˥', 'riŋ̟˨˩', 'tʰi-', 'nɯəŋ~˧˥', 'ʈ͡ʂe˧ˀ˥', 'ki˨˩', 'nullaːŋ̟-', 'kɔn-', 'mon˧˩', 'siəu-', 'raŋ˨˩', 't͡ɕi˧˥', 'dəːn˨˩', 'kuə˧˩', 'ŋan˧˥', 'ŋəːi˨˩', 'tiən~˧˥', 'ho˨˩', 'hun~˧˥', 'ʂan˧˥', 'ʝɯəi˧˥', 'su-', 'zuəŋ˨˩', 'faːi˧˩', 'ʈ͡ʂun-', 'təm~˧ˀ˩', 'tʰɔ˧˩', 'ʂiu˨˩', 'kɯə~˧ˀ˩', 'χuən~˧˥', 'lo-', 'don˧ˀ˩', 't͡ɕan~˧ˀ˩', 'ɣaːm~˧˥', 'kiəu˨˩', 't͡ɕan˧ˀ˥', 'kwaːn˧˩', 'ʂaːn~˧˥', 'vaːi-', 'sɯ˧˩', 'vəːi˧˥', 'nulləː-', 'nullu-', 'biəu˧˥', 'kɔn˨˩', 'ŋin˨˩', 'he˧ˀ˩', 'tiəm-', 'ʂɯəŋ˧ˀ˩', 're~˧˥', 't͡ɕaːn~˧˥', 'tʰuŋ-', 'ɲən˧ˀ˩', 'baː-', 'ŋɛu˧ˀ˩', 'kwəi-', 'lɯu-', 'kɯu˧ˀ˥', 'lɯəi˧˥'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# num <unk> in test result\n",
    "test_path_result = \"./result/result-tasa-w2i-lsvsc.json\"\n",
    "test_result = load_json(test_path_result)\n",
    "num_unk = 0\n",
    "idx_list_data = []\n",
    "\n",
    "for idx, item in enumerate(test_result):\n",
    "    try:\n",
    "        transcription = item[\"gold\"]\n",
    "        for idx_word, token in enumerate(transcription.split()):\n",
    "            if token == \"<unk>\":\n",
    "                num_unk += 1\n",
    "                idx_list_data.append((idx, idx_word))\n",
    "    except: \n",
    "        continue\n",
    "\n",
    "\n",
    "# num correct fill by phoneme based model \n",
    "\n",
    "test_result_phoneme = load_json(\"./result/result-tasa-p2i_lsvsc.json\")\n",
    "phoneme_test_path = \"../dataset/LSVSC_test_phoneme.json\"\n",
    "phoneme_test_data = load_json(phoneme_test_path)\n",
    "num_correct_fill = 0\n",
    "\n",
    "correct_filled = []\n",
    "cannot_be_filled = []\n",
    "tried_to_filled_but_wrong = []\n",
    "total_num = 0\n",
    "for idx, idx_word in idx_list_data:\n",
    "    if idx_word < len(test_result_phoneme[idx][\"predicted\"].split(' ')):\n",
    "        gold = test_result_phoneme[idx][\"gold\"].split(' ')[idx_word]\n",
    "        pred = test_result_phoneme[idx][\"predicted\"].split(' ')[idx_word]\n",
    "        # print(f\"Gold: {gold}, Pred: {pred}\")\n",
    "        if gold == pred:\n",
    "            correct_filled.append(gold)\n",
    "            num_correct_fill += 1\n",
    "        else:\n",
    "            cannot_be_filled.append(gold)\n",
    "            tried_to_filled_but_wrong.append(pred)\n",
    "        total_num += 1\n",
    "\n",
    "print(f\"Number of OOV words: {len(oov_word)}, Percentage: {len(oov_word)/len(unique_token_test)*100:.2f}%\")\n",
    "print(f\"Number of <unk> in transcript of test dataset: {total_num}\")\n",
    "print(f\"Number of correct fill by phoneme based model: {num_correct_fill}\")\n",
    "print(f\"Percentage of correct fill by phoneme based model: {num_correct_fill/total_num*100:.2f}%\")\n",
    "print(f\"Correctly filled words: {set(correct_filled)}\")\n",
    "print(f\"Cannot be filled words: {set(cannot_be_filled)}\")\n",
    "print(f\"Tried to fill but wrong predictions: {set(tried_to_filled_but_wrong)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "512e6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(path, data):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a json file.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"w\", encoding= 'utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "save_json(\"./result/correctly_filled_words.json\", list(set(correct_filled)))\n",
    "save_json(\"./result/cannot_be_filled_words.json\", list(set(cannot_be_filled)))\n",
    "save_json(\"./result/tried_to_filled_but_wrong_predictions.json\", list(set(tried_to_filled_but_wrong)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffb4d3",
   "metadata": {},
   "source": [
    "## Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
