{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f25b006",
   "metadata": {},
   "source": [
    "## Phoneme Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f497fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words: 116, Percentage: 3.78%\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "def load_json(path):\n",
    "    \"\"\"\n",
    "    Load a json file and return the content as a dictionary.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"r\", encoding= 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_unique_tokens(data):\n",
    "    \"\"\"\n",
    "    Get unique tokens from the dataset.\n",
    "    \"\"\"\n",
    "    unique_tokens = set()\n",
    "    for item in data:\n",
    "        transcription = item[\"text\"]\n",
    "        for token in transcription.split():\n",
    "            unique_tokens.add(token)\n",
    "    return list(unique_tokens)\n",
    "\n",
    "testset_path= \"../dataset/LSVSC_test_word.json\"\n",
    "vocab_train_path = \"../dataset/word_vocab_lsvsc.json\"\n",
    "\n",
    "vocab_train = load_json(vocab_train_path)\n",
    "unique_token_test = get_unique_tokens(load_json(testset_path))\n",
    "oov_word = []\n",
    "for token in unique_token_test:\n",
    "    if token not in vocab_train:\n",
    "        oov_word.append(token)\n",
    "\n",
    "print(f\"Number of OOV words: {len(oov_word)}, Percentage: {len(oov_word)/len(unique_token_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9393d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold phoneme: χi- den˧˥ χu- vɯŋ~˧ˀ˩ nulləm~˧˥ doŋ˨˩ ki˧ˀ˩ saː˧ˀ˥ kwoŋ~˧˥ tʰaːi˧˥ hwiən˧ˀ˩ nullaːn- fu˧˥ bi˧ˀ˩ to˧˩ koŋ- taːŋ~˧˥ liən- ŋaːŋ̟˨˩ t͡ɕoŋ˧˥ buən- ləu˧ˀ˩ tiŋ̟˧˩ faːn~˧˥ hiən˧ˀ˩ ban~˧˥ zɯ˧ˀ˥ | pred phonem: χi- den˧˥ χu- vɯŋ~˧ˀ˩ nulləm~˧˥ doŋ˨˩ ki˧ˀ˩ saː˧ˀ˥ kwoŋ~˧˥ tʰaːi˧˥ hwiən˧ˀ˩ nullaːn- fu˧˥ bi˧ˀ˩ to˧˩ koŋ- taːŋ~˧˥ liən- ŋaːŋ̟˨˩ t͡ɕoŋ˧˥ buən- ləu˧ˀ˩ tiŋ̟˧˩ faːn~˧˥ hiən˧ˀ˩ ban~˧˥ zɯ˧ˀ˥\n",
      "Gold phoneme: tʰɛu- baː˨˩ faːm˧ˀ˩ tʰi˧ˀ˩ ʝuŋ- to˧˩ ʈ͡ʂɯəŋ˧˩ to˧˩ hwaː˨˩ zaːi˧˩ tʰaːŋ̟- laːn˧ˀ˩ saː˧ˀ˥ tʰaːŋ̟- tiŋ̟˧ˀ˩ biŋ̟˨˩ hwiən˧ˀ˩ dɯŋ~˧˥ tʰɔ˧ˀ˩ tiŋ̟˧˩ haː˨˩ tiŋ̟˧ˀ˥ | pred phonem: tʰɛu- baː˨˩ faːm˧ˀ˩ tʰi˧ˀ˩ ʝuŋ- to˧˩ ʈ͡ʂɯəŋ˧˩ to˧˩ hwaː˨˩ zaːi˧˩ tʰaːŋ̟- laːn˧ˀ˩ saː˧ˀ˥ tʰaːŋ̟- tiŋ̟˧ˀ˩ biŋ̟˨˩ hwiən˧ˀ˩ dɯŋ~˧˥ tʰɔ˧ˀ˩ tiŋ̟˧˩ haː˨˩ tiŋ̟˧ˀ˥\n",
      "Gold phoneme: mɛ˧ˀ˩ vaː˨˩ t͡ɕi˧ˀ˩ nɔ˧˥ daːŋ- ŋoi˨˩ vaː˧˥ lɯəi˧˥ ben- məi˧˥ t͡ɕiəŋ~˧˥ tʰwiən˨˩ tʰuŋ˧˥ voi˧ˀ˩ vaːŋ˨˩ t͡ɕaːi˧ˀ˩ len- t͡ɕo˧ˀ˥ hɯŋ- nam˨˩ | pred phonem: mɛ˧ˀ˩ vaː˨˩ t͡ɕi˧ˀ˩ nɔ˧˥ daːŋ- ŋoi˨˩ vaː˧˥ lɯəi˧˥ ben- məi˧˥ t͡ɕiəŋ~˧˥ tʰwiən˨˩ tʰuŋ˧˥ voi˧ˀ˩ vaːŋ˨˩ t͡ɕaːi˧ˀ˩ len- t͡ɕo˧ˀ˥ hɯŋ- nam˨˩\n",
      "Gold phoneme: kuŋ˧ˀ˥ ɲɯ- ɲɯŋ˧ˀ˥ ŋɯəi˨˩ kɔn˨˩ ŋəː˨˩ vɯŋ~˧ˀ˩ χaːŋ~˧˥ ʈ͡ʂɔŋ- daːm˧˥ doŋ- nullaːŋ̟- daː˧ˀ˥ ɲin˨˩ tʰəi˧˥ rɔ˧ˀ˥ kaːi˧˥ ʈ͡ʂɔ˨˩ mi˧ˀ˩ ʝən- kuə˧˩ ten- t͡ɕi˧˩ hwi- ʈ͡ʂɯəŋ˧˩ | pred phonem: kuŋ˧ˀ˥ ɲɯ- ɲɯŋ˧ˀ˥ ŋɯəi˨˩ kɔn˨˩ ŋəː˨˩ vɯŋ~˧ˀ˩ χaːŋ~˧˥ ʈ͡ʂɔŋ- daːm˧˥ doŋ- nullaːŋ̟- daː˧ˀ˥ ɲin˨˩ tʰəi˧˥ rɔ˧ˀ˥ kaːi˧˥ ʈ͡ʂɔ˨˩ mi˧ˀ˩ ʝən- kuə˧˩ ten- t͡ɕi˧˩ hwi- ʈ͡ʂɯəŋ˧˩\n",
      "Number of OOV words: 116, Percentage: 3.78%\n",
      "Number of <unk> in transcript of test dataset: 132\n",
      "Number of correct fill by phoneme based model: 29\n",
      "Percentage of correct fill by phoneme based model: 21.97%\n",
      "Correctly filled words: {'ɣəː-', 'vaːŋ̟˧ˀ˩', 'ʂiu˧˩', 'tʰuŋ˧˥', 'lan~˧ˀ˩', 'tʰɔm~˧˥', 'ŋaːi˧˥', 'ŋɔn˨˩', 'vaːi˧˥', 'χɯən~˧˥', 'həːn˧˥', 'tʰwi˧˩', 'vɔi-', 'ʈ͡ʂɛu˧˩', 'kwiən~˧˥', 'biu˧˥', 'ren˨˩', 'son˧˥', 'muəŋ˧˥', 'bɔ˧ˀ˩', 'ki˧ˀ˩', 'vaːm˧ˀ˩', 't͡ɕwɛ˧˥', 'haːŋ˧˥', 'mi˧ˀ˩', 'laːn˧ˀ˩', 'ʂɔn˨˩'}\n",
      "Cannot be filled words: {'fi˧ˀ˩', 'χaŋ˧˥', 'saːi-', 'neŋ̟~˧˥', 'san˧˥', 'tʰɯk-', 'nən˧˥', 'ɲan˧ˀ˩', 'χəːi˧˥', 'lwaːn~˧˥', 'ʂəi˧˩', 'ɣwəi-', 'nɯu-', 'biu˧˥', 'tʰiə-', 'lu˨˩', 'səu-', 'mɛn~˧ˀ˩', 'nullwan˨˩', 'vɔi˧˥', 'rəːm˨˩', 'kɯəŋ~˧ˀ˩', 'hut-', 'kiəŋ˨˩', 'vaːn~˧˥', 'muəi˨˩', 'ʂiə˧ˀ˩', 'mɯə˧˥', 'lɯə-', 'ʝik̟-', 'ʈ͡ʂɯəŋ˧ˀ˩', 'tʰum-', 'raŋ˧˥', 'bɯəŋ-', 'kəŋ~˧˥', 'ɣaːp-', 'ŋwɛu˧ˀ˩', 'tɯn~˧˥', 'kiəŋ˧ˀ˥', 'sun-', 'bɯn-', 'kiəu˧ˀ˩', 'hwɛ˨˩', 'ŋen˧˩', 'fɛŋ-', 't͡ɕɔŋ~˧˥', 'ɲɯ˧˩', 'huə˧˩', 'ɣəː-', 'ɣi˨˩', 'hon˧˩', 'nəːm-', 'vɔi-', 't͡ɕum-', 't͡ɕaːŋ-', 'sum-', 'ruəŋ˨˩', 'tʰaːi˧˥', 'tʰɯən˨˩', 't͡ɕɯəŋ˨˩', 'ʂɔːŋ~˧˥', 'ɣen~˧˥', 'vɯə-', 'ruŋ˧˥', 'kɔːŋ-', 'raːn˧˥', 'haːŋ~˧ˀ˩', 't͡ɕe˧ˀ˥', 'veu-', 'ko˨˩', 'χɯən~˧˥', 'maːn~˧ˀ˩', 'zɔi˨˩', 'tʰwəː˧˩', 'faːm˧˩', 'ʂaːt-', 'non˧ˀ˩', 'nullui-', 'ɣəːm˨˩', 't͡ɕwaːŋ˧˩', 'ɲaːm~˧˥', 'ʝiu˨˩', 'ʈ͡ʂɔn~˧˥', 'ɲəm˧˩', 'ŋun~˧˥', 'he˧ˀ˥', 't͡ɕaːn~˧ˀ˩', 'nəːm˧˥', 'nəŋ˧˥', 'rɯəi˧˥', 'nullɯŋ˧ˀ˥', 'tʰum˨˩', 'ŋin-'}\n",
      "Tried to fill but wrong predictions: {'raŋ˨˩', 't͡ɕan~˧ˀ˩', 'ɣaːm~˧˥', 'toi-', 'biəu˧˥', 'tʰɯŋ~˧ˀ˩', 'səː˨˩', 'ʂɔn~˧˥', 'de˧ˀ˩', 'ʂəu-', 'hun~˧˥', 'ʂɯŋ~˧ˀ˩', 'hɔ˨˩', 'nɯəŋ~˧˥', 'vəːi˧˥', 't͡ɕan˧ˀ˥', 'siu˨˩', 'vi˧ˀ˩', 'kwaːn˨˩', 'mɯəŋ-', 'kiəu˧˩', 'baːŋ~˧ˀ˩', 'tʰuŋ-', 'nəm-', 'nen˨˩', 'hwe˨˩', 'ɲəm˨˩', 't͡ɕam~˧˥', 'lon~˧˥', 'ɲaːu-', 'ɣɔi˨˩', 'tʰi˨˩', 'ŋɛu˧ˀ˩', 'nɛn~˧˥', 'lɯən~˧˥', 'kuə˧˩', 'tin~˧˥', 'kaːŋ~˧˥', 've-', 'kiən˧ˀ˥', 'ɲɯ˨˩', 'ɣaːm-', 'ʈ͡ʂɯəŋ˨˩', 'dɯŋ˧˥', 'ʂaːŋ~˧˥', 'nulliəu-', 'nullaːŋ̟-', 'rɔi˨˩', 'tʰi-', 'kəː˨˩', 'ʝiŋ̟~˧ˀ˩', 'tʰɯəŋ˨˩', 'lɯu-', 'te-', 'ʂan˧˥', 'suŋ-', 'nulli˨˩', 'lo-', 'lɯəi˧˥', 'ʂi˧ˀ˩', 'nan˧˥', 'ɲaːn~˧˥', 'ʝaːn˧˥', 'kɔn-', 'ʈ͡ʂaːŋ-', 'ʝɯəi˧˥', 'sɯəŋ˧ˀ˩', 'ɲən˧ˀ˩', 'ʂaːŋ̟~˧ˀ˩', 'tʰuŋ˨˩', 'kaːi˧˥', 'ʈ͡ʂe˧ˀ˥', 'ŋɯəi˨˩', 'he˧ˀ˩', 'laː˨˩', 'raːn-', 't͡ɕon-', 'zuəŋ˨˩', 'daːŋ˧˥', 'tʰɔ˧˩', 'men~˧ˀ˩', 't͡ɕim-', 'saːi˧˩', 'tuŋ~˧ˀ˩', 'man~˧ˀ˩', 'vaːi-', 'ŋin˨˩', 'ɣɯi˧˩', 'kuəŋ~˧ˀ˩', 'lon˧ˀ˩', 'muən~˧˥', 'kiəm˧˩', 't͡ɕaːn~˧˥', 'noŋ-', 'χaŋ˧˩', 'de˧˩', 'ɣwəi˨˩', 'ʈ͡ʂɔŋ-', 'tʰe˧˩'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# num <unk> in test result\n",
    "test_path_result = \"/home/anhkhoa/PhonoASR/result_new/result-vggTransformer-w2i-lsvsc.json\"\n",
    "test_result = load_json(test_path_result)\n",
    "num_unk = 0\n",
    "idx_list_data = []\n",
    "\n",
    "for idx, item in enumerate(test_result):\n",
    "    try:\n",
    "        transcription = item[\"gold\"]\n",
    "        for idx_word, token in enumerate(transcription.split()):\n",
    "            if token == \"<unk>\":\n",
    "                num_unk += 1\n",
    "                idx_list_data.append((idx, idx_word))\n",
    "    except: \n",
    "        continue\n",
    "\n",
    "\n",
    "# num correct fill by phoneme based model \n",
    "\n",
    "test_result_phoneme = load_json(\"/home/anhkhoa/PhonoASR/result_new/result-tasa-p2i-lsvsc.json\")\n",
    "\n",
    "num_correct_fill = 0\n",
    "\n",
    "correct_filled = []\n",
    "cannot_be_filled = []\n",
    "tried_to_filled_but_wrong = []\n",
    "test_data = load_json(testset_path)\n",
    "sample_correct = []\n",
    "total_num = 0\n",
    "for idx, idx_word in idx_list_data:\n",
    "    if idx_word < len(test_result_phoneme[idx][\"predicted\"].split(' ')):\n",
    "        gold = test_result_phoneme[idx][\"gold\"].split(' ')[idx_word]\n",
    "        pred = test_result_phoneme[idx][\"predicted\"].split(' ')[idx_word]\n",
    "        # print(f\"Gold: {gold}, Pred: {pred}\")\n",
    "        if gold == pred:\n",
    "            \n",
    "            correct_filled.append(gold)\n",
    "            num_correct_fill += 1\n",
    "            if test_result_phoneme[idx]['gold'] == test_result_phoneme[idx][\"predicted\"]:\n",
    "                print(f\"Gold phoneme: {test_result_phoneme[idx]['gold']} | pred phonem: {test_result_phoneme[idx]['predicted']}\")\n",
    "\n",
    "                sample_correct.append([\n",
    "                    test_result[idx]['gold'],\n",
    "                    test_data[idx]['text']\n",
    "                ])\n",
    "        else:\n",
    "            cannot_be_filled.append(gold)\n",
    "            tried_to_filled_but_wrong.append(pred)\n",
    "        total_num += 1\n",
    "\n",
    "print(f\"Number of OOV words: {len(oov_word)}, Percentage: {len(oov_word)/len(unique_token_test)*100:.2f}%\")\n",
    "print(f\"Number of <unk> in transcript of test dataset: {total_num}\")\n",
    "print(f\"Number of correct fill by phoneme based model: {num_correct_fill}\")\n",
    "print(f\"Percentage of correct fill by phoneme based model: {num_correct_fill/total_num*100:.2f}%\")\n",
    "print(f\"Correctly filled words: {set(correct_filled)}\")\n",
    "print(f\"Cannot be filled words: {set(cannot_be_filled)}\")\n",
    "print(f\"Tried to fill but wrong predictions: {set(tried_to_filled_but_wrong)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c3613d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold unknown word: tú quyên thở dài <unk> thượt \n",
      "Predicted Phoneme Converted: tú quyên thở dài thườn thượt \n",
      "--------------------\n",
      "Gold unknown word: hai tay chống nạnh hắn đứng dạng <unk> trước phòng làm việc \n",
      "Predicted Phoneme Converted: hai tay chống nạnh hắn đứng dạng háng trước phòng làm việc \n",
      "--------------------\n",
      "Gold unknown word: đêm vừa rồi một tên trong đám <unk> <unk> đã chết về tay hắn \n",
      "Predicted Phoneme Converted: đêm vừa rồi một tên trong đám giòi bọ đã chết về tay hắn \n",
      "--------------------\n",
      "Gold unknown word: mẹ và chị nó đang ngồi vá lưới bên mấy chiếc thuyền <unk> vội vàng chạy lên chỗ hưng nằm \n",
      "Predicted Phoneme Converted: mẹ và chị nó đang ngồi vá lưới bên mấy chiếc thuyền thúng vội vàng chạy lên chỗ hưng nằm \n",
      "--------------------\n",
      "Gold unknown word: cũng như những người còn ngờ vực khác trong đám đông anh đã nhìn thấy rõ cái trò <unk> dân của tên chỉ huy trưởng \n",
      "Predicted Phoneme Converted: cũng như những người còn ngờ vực khác trong đám đông anh đã nhìn thấy rõ cái trò mị dân của tên chỉ huy trưởng \n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "duplicate_check = set()\n",
    "for gold, text in sample_correct:\n",
    "    if (gold, text) not in duplicate_check:\n",
    "        print(f\"Gold unknown word: {gold} \\nPredicted Phoneme Converted: {text} \\n{'-'*20}\")\n",
    "        duplicate_check.add((gold, text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "512e6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(path, data):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a json file.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"w\", encoding= 'utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "save_json(\"./result/correctly_filled_words.json\", list(set(correct_filled)))\n",
    "save_json(\"./result/cannot_be_filled_words.json\", list(set(cannot_be_filled)))\n",
    "save_json(\"./result/tried_to_filled_but_wrong_predictions.json\", list(set(tried_to_filled_but_wrong)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffb4d3",
   "metadata": {},
   "source": [
    "## Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f90ec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens correctly predicted at least 1 times: 2440\n",
      "Total unique tokens in GT: 2957\n",
      "Total words in GT: 138868\n",
      "Total words accounted in stats: 138868\n",
      "Total words correctly predicted in GT: 126306\n",
      "Percent correct: 90.95%\n",
      "Words used: 2957\n",
      "Pearson r: 0.23126171412486463\n",
      "Spearman rho: 0.3899768691222494\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load unique word vocab \n",
    "# load unique word phoneme \n",
    "\n",
    "def load_json(path):\n",
    "    \"\"\"\n",
    "    Load a json file and return the content as a dictionary.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"r\", encoding= 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def save_json(path, data):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a json file.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(path, \"w\", encoding= 'utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "from jiwer import process_words\n",
    "\n",
    "def compute_token_stats_with_jiwer(path, correct_at_least=1, save_path=None):\n",
    "    data = load_json(path)\n",
    "    data = [x for x in data if isinstance(x, dict) and \"gold\" in x and \"predicted\" in x]\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    for item in data:\n",
    "        gold = item[\"gold\"].strip()\n",
    "        pred = item[\"predicted\"].strip()\n",
    "\n",
    "        out = process_words(gold, pred)\n",
    "\n",
    "        aligns = out.alignments[0]\n",
    "\n",
    "        # unwrap references/hypotheses if nested\n",
    "        ref = out.references[0]     # list[str]\n",
    "\n",
    "        for ch in aligns:\n",
    "            op = ch.type\n",
    "            rs, re = ch.ref_start_idx, ch.ref_end_idx\n",
    "\n",
    "            if op == \"insert\":\n",
    "                continue\n",
    "\n",
    "            # for equal/delete/substitute: total increases for ref tokens\n",
    "            for tok in ref[rs:re]:\n",
    "                # tok must be a string token\n",
    "                if isinstance(tok, list):\n",
    "                    # unexpected nesting; flatten\n",
    "                    for t in tok:\n",
    "                        stats.setdefault(t, {\"total\": 0, \"correct\": 0})\n",
    "                        stats[t][\"total\"] += 1\n",
    "                        if op == \"equal\":\n",
    "                            stats[t][\"correct\"] += 1\n",
    "                else:\n",
    "                    stats.setdefault(tok, {\"total\": 0, \"correct\": 0})\n",
    "                    stats[tok][\"total\"] += 1\n",
    "                    if op == \"equal\":\n",
    "                        stats[tok][\"correct\"] += 1\n",
    "\n",
    "    unique_correct = sum(1 for tok, s in stats.items() if s[\"correct\"] >= correct_at_least)\n",
    "\n",
    "    print(f\"Number of unique tokens correctly predicted at least {correct_at_least} times: {unique_correct}\")\n",
    "    print(f\"Total unique tokens in GT: {len(stats)}\")\n",
    "\n",
    "    # stats = {tok: s for tok, s in stats.items() if s[\"correct\"] >= correct_at_least}\n",
    "    if save_path:\n",
    "        save_json(save_path, stats)\n",
    "    \n",
    "    cnt = 0\n",
    "    for item in data:\n",
    "        gold = item[\"gold\"].strip()\n",
    "        cnt += len(gold.split())\n",
    "\n",
    "    total_words = sum([s[\"total\"] for s in stats.values()])\n",
    "    print(f\"Total words in GT: {cnt}\")\n",
    "    print(f\"Total words accounted in stats: {total_words}\")\n",
    "\n",
    "\n",
    "    total_correct = sum([s[\"correct\"] for s in stats.values()])\n",
    "\n",
    "    print(f\"Total words correctly predicted in GT: {total_correct}\")\n",
    "    print(f\"Percent correct: {total_correct/cnt*100:.2f}%\")\n",
    "    return stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_frequency_training_set(path, save_path = None, type = 'phoneme'):\n",
    "    data = load_json(path)\n",
    "    word_count = {}\n",
    "    for item in data:\n",
    "        \n",
    "        transcription = item[\"phoneme_presentation\"] if type == 'phoneme' else item[\"text\"]\n",
    "        for token in transcription.split(' '):\n",
    "            if token not in word_count:\n",
    "                word_count[token] = 0\n",
    "            word_count[token] += 1\n",
    "    if save_path:\n",
    "        save_json(save_path, word_count)\n",
    "    return word_count\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def build_vectors(test_stats, train_freq, include_zero=False):\n",
    "    freqs, recalls = [], []\n",
    "\n",
    "    for w, s in test_stats.items():\n",
    "        if s[\"total\"] == 0:\n",
    "            continue\n",
    "\n",
    "        recall = s[\"correct\"] / s['total']\n",
    "        freq = train_freq.get(w, 0)\n",
    "        \n",
    "\n",
    "        if not include_zero and freq == 0:\n",
    "            continue\n",
    "\n",
    "        freqs.append(freq)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return np.array(freqs), np.array(recalls)\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "unique_word_count_test_set = compute_token_stats_with_jiwer(\"./result/result-tasa-w2i-lsvsc.json\", correct_at_least=1, save_path=\"./result/unique_word_stats_test_set.json\")\n",
    "unique_word_count_training_set = word_frequency_training_set(\"../dataset/LSVSC_train_word.json\", save_path=\"./result/unique_word_stats_training_set.json\", type = 'word')\n",
    "\n",
    "freqs, recalls = build_vectors(\n",
    "    unique_word_count_test_set,\n",
    "    unique_word_count_training_set,\n",
    "    include_zero=True   # giống paper\n",
    ")\n",
    "\n",
    "pearson_r, _ = pearsonr(freqs, recalls)\n",
    "spearman_rho, _ = spearmanr(freqs, recalls)\n",
    "\n",
    "print(\"Words used:\", len(freqs))\n",
    "print(\"Pearson r:\", pearson_r)\n",
    "print(\"Spearman rho:\", spearman_rho)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d99e654",
   "metadata": {},
   "source": [
    "Result based on TASA p, c, word result\n",
    "\n",
    "## Phoneme\n",
    "Number of unique tokens correctly predicted at least 1 times: 2474\n",
    "Total unique tokens in GT: 3001\n",
    "Words used: 3001\n",
    "Pearson r: 0.22260085725351292\n",
    "Spearman rho: 0.32388628887549537\n",
    "\n",
    "## Character\n",
    "\n",
    "Number of unique tokens correctly predicted at least 1 times: 2700\n",
    "Total unique tokens in GT: 3586\n",
    "Words used: 3586\n",
    "Pearson r: 0.24836971446922626\n",
    "Spearman rho: 0.4811725802044207\n",
    "\n",
    "## Word\n",
    "\n",
    "Number of unique tokens correctly predicted at least 1 times: 2440\n",
    "Total unique tokens in GT: 2957\n",
    "Words used: 2957\n",
    "Pearson r: 0.23126171412486463\n",
    "Spearman rho: 0.3899768691222494"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
